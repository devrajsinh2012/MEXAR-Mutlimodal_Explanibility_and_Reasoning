{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhqlbpIi8cCn"
      },
      "source": [
        "# üß¨ MEXAR Nano: SympScan Integration\n",
        "\n",
        "\n",
        "**Dataset:** SympScan (Symptoms, Descriptions, Diets, Medications, Precautions, Workouts)\n",
        "\n",
        "This notebook trains the **Mexar Nano** model on the SympScan dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpiPn77Q9xKw",
        "outputId": "5ab137eb-637f-45b8-f580-2cdeecfce121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ej9z2y68cCp"
      },
      "source": [
        "## 1. Setup & Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYm0K45Z8cCq",
        "outputId": "57e92fdf-1d76-4e39-bd72-ac46c9197b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Distillation Hyperparameters\n",
        "TEMPERATURE = 4.0   # Softens the teacher's probability distribution\n",
        "ALPHA = 0.7         # Weight for Distillation Loss (0.7 from Teacher, 0.3 from Labels)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS_TEACHER = 40\n",
        "EPOCHS_STUDENT = 40 # Student often needs a bit more time to settle"
      ],
      "metadata": {
        "id": "-2PVFqThBzot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJz3jujL8cCq"
      },
      "source": [
        "## 2. Data Loading\n",
        "\n",
        "We load the main training data (`Diseases_and_Symptoms_dataset.csv`) and the supplementary knowledge base files.\n",
        "\n",
        "**Note:** Ensure your `archive.zip` is extracted so the CSV files are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXd2xPZe8cCr",
        "outputId": "cd4b1588-1295-4214-d6b3-e6f2e52b02c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/7] Processing Data...\n",
            "   - Main Dataset Loaded: (96088, 231)\n",
            "   - Detected 230 Symptoms\n",
            "   - Detected 100 Diseases\n"
          ]
        }
      ],
      "source": [
        "# --- CONFIGURATION ---\n",
        "\n",
        "print(\"\\n[1/7] Processing Data...\")\n",
        "#  Auto-extract if needed\n",
        "if os.path.exists(\"archive.zip\"):\n",
        "    with zipfile.ZipFile(\"archive.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "    print(\"   - Extracted archive.zip\")\n",
        "\n",
        "# Load Main Dataset\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Dataset/SympScan/Diseases_and_Symptoms_dataset.csv')\n",
        "    # Cleanup: Fill NaNs with 0\n",
        "    df = df.fillna(0)\n",
        "    print(f\"   - Main Dataset Loaded: {df.shape}\")\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(\"CRITICAL: 'Diseases_and_Symptoms_dataset.csv' not found.\")\n",
        "\n",
        "# Separate Features (Symptoms) and Target (Disease)\n",
        "# The dataset has 'diseases' in column 0, and symptoms in columns 1..N\n",
        "X_raw = df.iloc[:, 1:].values.astype(float)\n",
        "y_raw = df.iloc[:, 0].values\n",
        "\n",
        "symptom_names = list(df.columns[1:])\n",
        "print(f\"   - Detected {len(symptom_names)} Symptoms\")\n",
        "\n",
        "# Encode Targets\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_raw)\n",
        "disease_names = le.classes_\n",
        "print(f\"   - Detected {len(disease_names)} Diseases\")\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_raw, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch Dataset\n",
        "class MedicalDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "train_loader = DataLoader(MedicalDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(MedicalDataset(X_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L52FoumM8cCr"
      },
      "source": [
        "## 3. CONSTRUCTING KNOWLEDGE BASE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffwbhQZ08cCs",
        "outputId": "7576df0e-d3fd-4f59-a337-0278bddc2291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/7] Building Knowledge Base...\n",
            "   - Merged description (100 entries)\n",
            "   - Merged diets (100 entries)\n",
            "   - Merged medications (100 entries)\n",
            "   - Merged precautions (100 entries)\n",
            "   - Merged workout (100 entries)\n"
          ]
        }
      ],
      "source": [
        "# This merges all the helper CSVs into one dictionary for the app\n",
        "print(\"\\n[2/7] Building Knowledge Base...\")\n",
        "\n",
        "kb_files = {\n",
        "    'description': '/content/drive/MyDrive/Dataset/SympScan/description.csv',\n",
        "    'diets': '/content/drive/MyDrive/Dataset/SympScan/diets.csv',\n",
        "    'medications': '/content/drive/MyDrive/Dataset/SympScan/medications.csv',\n",
        "    'precautions': '/content/drive/MyDrive/Dataset/SympScan/precautions.csv',\n",
        "    'workout': '/content/drive/MyDrive/Dataset/SympScan/workout.csv'\n",
        "}\n",
        "\n",
        "full_knowledge_base = {}\n",
        "\n",
        "for key, fname in kb_files.items():\n",
        "    if os.path.exists(fname):\n",
        "        sub_df = pd.read_csv(fname)\n",
        "        # Normalize columns: remove whitespace, lowercase\n",
        "        sub_df.columns = [c.strip().lower() for c in sub_df.columns]\n",
        "\n",
        "        # We assume column 0 is the disease name key\n",
        "        key_col = sub_df.columns[0]\n",
        "\n",
        "        # Convert to dictionary: { \"DiseaseName\": {data...} }\n",
        "        full_knowledge_base[key] = sub_df.set_index(key_col).to_dict(orient='index')\n",
        "        print(f\"   - Merged {key} ({len(sub_df)} entries)\")\n",
        "    else:\n",
        "        print(f\"   - ‚ö†Ô∏è Missing {fname}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Zu-O9Zx8cCs"
      },
      "source": [
        "## 4. MODEL ARCHITECTURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyfy_Mn08cCs",
        "outputId": "2ca03db2-72b1-4fec-cdf6-b6e9771db592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[3/7] Architectures Initialized\n",
            "   - Teacher Params: 297188\n",
            "   - Student Params: 44324 (Compressed Version)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- TEACHER MODEL (Large, Powerful) ---\n",
        "class MexarTeacher(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MexarTeacher, self).__init__()\n",
        "        # Deeper, wider layers to learn complex patterns\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# --- STUDENT MODEL (Nano - The one we export) ---\n",
        "class MexarNano(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MexarNano, self).__init__()\n",
        "        # Shallow, narrow layers for speed and mobile efficiency\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_size, 128), # Standard mobile-friendly width\n",
        "            nn.ReLU(),\n",
        "            # No BatchNorm or Dropout in inference layers to keep it raw and fast,\n",
        "            # though sometimes helpful for training. We keep it simple.\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Instantiate\n",
        "input_dim = len(symptom_names)\n",
        "num_classes = len(disease_names)\n",
        "\n",
        "teacher = MexarTeacher(input_dim, num_classes).to(device)\n",
        "student = MexarNano(input_dim, num_classes).to(device)\n",
        "\n",
        "print(f\"\\n[3/7] Architectures Initialized\")\n",
        "print(f\"   - Teacher Params: {sum(p.numel() for p in teacher.parameters())}\")\n",
        "print(f\"   - Student Params: {sum(p.numel() for p in student.parameters())} (Compressed Version)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU8QaLFw8cCs"
      },
      "source": [
        "## 5. TRAINING THE TEACHER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg1opDUn8cCt",
        "outputId": "b1f3a16f-b15c-4ed8-abd4-37b7340955b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[4/7] Training Teacher Model (40 Epochs)...\n",
            "   Epoch 5: Loss 0.3067 | Acc 87.94%\n",
            "   Epoch 10: Loss 0.2687 | Acc 88.51%\n",
            "   Epoch 15: Loss 0.2503 | Acc 88.69%\n",
            "   Epoch 20: Loss 0.2375 | Acc 89.03%\n",
            "   Epoch 25: Loss 0.2328 | Acc 89.14%\n",
            "   Epoch 30: Loss 0.2262 | Acc 89.14%\n",
            "   Epoch 35: Loss 0.2211 | Acc 89.39%\n",
            "   Epoch 40: Loss 0.2165 | Acc 89.44%\n",
            "   ‚úÖ Teacher Training Complete.\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n[4/7] Training Teacher Model ({EPOCHS_TEACHER} Epochs)...\")\n",
        "\n",
        "optimizer_T = optim.Adam(teacher.parameters(), lr=0.001)\n",
        "criterion_T = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(EPOCHS_TEACHER):\n",
        "    teacher.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer_T.zero_grad()\n",
        "        outputs = teacher(inputs)\n",
        "        loss = criterion_T(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_T.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Validation\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"   Epoch {epoch+1}: Loss {running_loss/len(train_loader):.4f} | Acc {100*correct/total:.2f}%\")\n",
        "\n",
        "print(\"   ‚úÖ Teacher Training Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNP1NQ3u8cCt"
      },
      "source": [
        "## 6. KNOWLEDGE DISTILLATION (The Important Part)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAsuW9Js8cCt",
        "outputId": "f2bb8f2d-7fd8-46e5-825e-990ebf3db327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[5/7] Distilling Knowledge to Student (Mexar Nano)...\n",
            "   - Temperature: 4.0, Alpha: 0.7\n",
            "   Epoch 5: Loss 0.3949 | Acc 89.35%\n",
            "   Epoch 10: Loss 0.2685 | Acc 89.58%\n",
            "   Epoch 15: Loss 0.2348 | Acc 89.65%\n",
            "   Epoch 20: Loss 0.2180 | Acc 89.76%\n",
            "   Epoch 25: Loss 0.2066 | Acc 89.83%\n",
            "   Epoch 30: Loss 0.1987 | Acc 89.94%\n",
            "   Epoch 35: Loss 0.1929 | Acc 89.96%\n",
            "   Epoch 40: Loss 0.1887 | Acc 89.90%\n",
            "   ‚úÖ Distillation Complete.\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n[5/7] Distilling Knowledge to Student (Mexar Nano)...\")\n",
        "print(f\"   - Temperature: {TEMPERATURE}, Alpha: {ALPHA}\")\n",
        "\n",
        "optimizer_S = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "def distillation_loss(student_logits, teacher_logits, labels, T, alpha):\n",
        "    # 1. Soft Loss (KL Divergence between Student/Teacher probabilities)\n",
        "    # We use LogSoftmax on Student and Softmax on Teacher\n",
        "    soft_targets = F.softmax(teacher_logits / T, dim=1)\n",
        "    student_log_soft = F.log_softmax(student_logits / T, dim=1)\n",
        "\n",
        "    # KLDivLoss expects input to be log-probs, target to be probs\n",
        "    distill_loss = nn.KLDivLoss(reduction='batchmean')(student_log_soft, soft_targets) * (T * T)\n",
        "\n",
        "    # 2. Hard Loss (Standard CrossEntropy with actual labels)\n",
        "    student_hard_loss = F.cross_entropy(student_logits, labels)\n",
        "\n",
        "    # Combine\n",
        "    return alpha * distill_loss + (1.0 - alpha) * student_hard_loss\n",
        "\n",
        "teacher.eval() # Teacher is frozen now\n",
        "student.train()\n",
        "\n",
        "for epoch in range(EPOCHS_STUDENT):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Get Teacher's \"Opinion\" (No grad needed)\n",
        "        with torch.no_grad():\n",
        "            teacher_logits = teacher(inputs)\n",
        "\n",
        "        # Get Student's Prediction\n",
        "        optimizer_S.zero_grad()\n",
        "        student_logits = student(inputs)\n",
        "\n",
        "        # Calculate Distillation Loss\n",
        "        loss = distillation_loss(student_logits, teacher_logits, labels, TEMPERATURE, ALPHA)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer_S.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(student_logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"   Epoch {epoch+1}: Loss {running_loss/len(train_loader):.4f} | Acc {100*correct/total:.2f}%\")\n",
        "\n",
        "print(\"   ‚úÖ Distillation Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gi3vDIP8cCt"
      },
      "source": [
        "## 7. FINAL EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "athz6ySG8cCu",
        "outputId": "17e41039-c87c-4306-c7a4-0011b3ba23ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6/7] Comparing Models on Test Data...\n",
            "   - Teacher (Heavy) Accuracy: 88.71%\n",
            "   - Student (Nano) Accuracy: 88.64%\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[6/7] Comparing Models on Test Data...\")\n",
        "\n",
        "def evaluate_model(model, loader, name):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"   - {name} Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate_model(teacher, test_loader, \"Teacher (Heavy)\")\n",
        "evaluate_model(student, test_loader, \"Student (Nano)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ MODEL TEST"
      ],
      "metadata": {
        "id": "ZrZM8Ch4IQHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==========================================\n",
        "# 1. DEFINE TREATMENT ENGINE\n",
        "# ==========================================\n",
        "class TreatmentEngine:\n",
        "    def __init__(self, diets_path, meds_path, precautions_path, workout_path):\n",
        "        # Load all auxiliary files\n",
        "        self.diets = pd.read_csv(\"/content/drive/MyDrive/Dataset/SympScan/diets.csv\")\n",
        "        self.meds = pd.read_csv(\"/content/drive/MyDrive/Dataset/SympScan/medications.csv\")\n",
        "        self.precautions = pd.read_csv(\"/content/drive/MyDrive/Dataset/SympScan/precautions.csv\")\n",
        "        self.workout = pd.read_csv(\"/content/drive/MyDrive/Dataset/SympScan/workout.csv\")\n",
        "\n",
        "        # Normalize disease column names for matching\n",
        "        # (Assuming first column is always the disease name)\n",
        "        self.diets.columns = ['Disease', 'Diet']\n",
        "        self.meds.columns = ['Disease', 'Medication']\n",
        "        self.precautions.columns = ['Disease', 'Precaution_1', 'Precaution_2', 'Precaution_3', 'Precaution_4']\n",
        "        self.workout.columns = ['Disease', 'Workout']\n",
        "\n",
        "    def get_plan(self, disease_name):\n",
        "        \"\"\"Fetches the full care plan for a predicted disease.\"\"\"\n",
        "        plan = {}\n",
        "\n",
        "        # Helper to get value safely\n",
        "        def get_val(df, col):\n",
        "            # Case-insensitive match\n",
        "            row = df[df['Disease'].str.lower() == disease_name.lower()]\n",
        "            return row[col].values[0] if not row.empty else \"Not specified.\"\n",
        "\n",
        "        # Fetch details\n",
        "        plan['Diet'] = get_val(self.diets, 'Diet')\n",
        "        plan['Meds'] = get_val(self.meds, 'Medication')\n",
        "        plan['Workout'] = get_val(self.workout, 'Workout')\n",
        "\n",
        "        # Precautions are spread across 4 columns\n",
        "        p_row = self.precautions[self.precautions['Disease'].str.lower() == disease_name.lower()]\n",
        "        if not p_row.empty:\n",
        "            plan['Precautions'] = [\n",
        "                p_row['Precaution_1'].values[0],\n",
        "                p_row['Precaution_2'].values[0],\n",
        "                p_row['Precaution_3'].values[0],\n",
        "                p_row['Precaution_4'].values[0]\n",
        "            ]\n",
        "        else:\n",
        "            plan['Precautions'] = []\n",
        "\n",
        "        return plan\n",
        "\n",
        "# ==========================================\n",
        "# 2. INITIALIZE THE DOCTOR AI\n",
        "# ==========================================\n",
        "# Note: Ensure these paths match where your files are.\n",
        "# Based on your notebook, they might be in 'archive.zip/' or '/content/drive/...'\n",
        "# I will try the standard extracted paths first.\n",
        "try:\n",
        "    doctor_ai = TreatmentEngine(\n",
        "        \"/content/drive/MyDrive/Dataset/SympScan/diets.csv\",\n",
        "        \"/content/drive/MyDrive/Dataset/SympScan/medications.csv\",\n",
        "        \"/content/drive/MyDrive/Dataset/SympScan/precautions.csv\",\n",
        "        \"/content/drive/MyDrive/Dataset/SympScan/workout.csv\"\n",
        "    )\n",
        "    print(\"‚úÖ Doctor AI (Treatment Engine) initialized successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ö†Ô∏è Files not found in 'archive.zip/'. Trying Drive paths from your notebook...\")\n",
        "    # Fallback to the paths seen in your notebook variables\n",
        "    doctor_ai = TreatmentEngine(\n",
        "        kb_files['diets'],\n",
        "        kb_files['medications'],\n",
        "        kb_files['precautions'],\n",
        "        kb_files['workout']\n",
        "    )\n",
        "    print(\"‚úÖ Doctor AI initialized using Drive paths.\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. DEFINE PREDICTION FUNCTIONS\n",
        "# ==========================================\n",
        "\n",
        "def predict_and_explain_patient(model, symptom_list, symptom_names, disease_names):\n",
        "    \"\"\"\n",
        "    Takes a list of symptoms (strings), predicts the disease,\n",
        "    and explains which symptoms drove the decision.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Create a blank feature vector\n",
        "    input_vector = torch.zeros(1, len(symptom_names)).to(device)\n",
        "\n",
        "    # 2. Map input strings to the feature vector\n",
        "    dataset_symptoms = [str(s).lower().strip() for s in symptom_names]\n",
        "    found_symptoms = []\n",
        "\n",
        "    print(f\"\\n--- Analyzing Symptoms: {symptom_list} ---\")\n",
        "    for s in symptom_list:\n",
        "        s_clean = s.lower().strip()\n",
        "\n",
        "        # exact match check\n",
        "        if s_clean in dataset_symptoms:\n",
        "            idx = dataset_symptoms.index(s_clean)\n",
        "            input_vector[0, idx] = 1.0\n",
        "            found_symptoms.append(s_clean)\n",
        "        else:\n",
        "            print(f\"  [!] Warning: Symptom '{s}' not found in dataset. Ignoring.\")\n",
        "\n",
        "    if len(found_symptoms) == 0:\n",
        "        print(\"  Error: No valid symptoms provided.\")\n",
        "        return None\n",
        "\n",
        "    # 3. Forward Pass\n",
        "    outputs = model(input_vector)\n",
        "    if isinstance(outputs, tuple):\n",
        "        logits = outputs[0]\n",
        "    else:\n",
        "        logits = outputs\n",
        "\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "    # Get Top-1 Prediction\n",
        "    top_prob, top_idx = probs.topk(1)\n",
        "    predicted_disease = disease_names[top_idx.item()]\n",
        "    confidence = top_prob.item() * 100\n",
        "\n",
        "    print(f\"  >> Diagnosis: {predicted_disease.upper()} ({confidence:.2f}%)\")\n",
        "    return predicted_disease\n",
        "\n",
        "def diagnose_and_prescribe(model, symptom_list):\n",
        "    # 1. Diagnose\n",
        "    disease_name = predict_and_explain_patient(model, symptom_list, train_ds.symptom_names, train_ds.disease_names)\n",
        "\n",
        "    if disease_name:\n",
        "        # 2. Prescribe\n",
        "        plan = doctor_ai.get_plan(disease_name)\n",
        "        print(f\"\\n--- üíä TREATMENT PLAN FOR: {disease_name.upper()} ---\")\n",
        "        print(f\"ü•ó Diet:        {plan['Diet']}\")\n",
        "        print(f\"üíä Medication:  {plan['Meds']}\")\n",
        "        print(f\"üèÉ Workout:     {plan['Workout']}\")\n",
        "\n",
        "        # Format precautions nicely\n",
        "        precautions = [str(p) for p in plan['Precautions'] if str(p) != 'nan']\n",
        "        print(f\"‚ö†Ô∏è Precautions: {', '.join(precautions)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmGJkHDHI9zu",
        "outputId": "6693da32-22f1-4a0e-b4fd-586b5a426619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Doctor AI (Treatment Engine) initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Define the Dataset Class (So Python knows how to read your file)\n",
        "class SymptomDiseaseDataset(Dataset):\n",
        "    def __init__(self, csv_path, label_encoder=None):\n",
        "        # Load the new SympScan dataset\n",
        "        # We try to handle different delimiters just in case\n",
        "        try:\n",
        "            data = pd.read_csv(\"/content/drive/MyDrive/Dataset/SympScan/Diseases_and_Symptoms_dataset.csv\")\n",
        "        except:\n",
        "            print(\"‚ö†Ô∏è Error reading CSV. Checking path...\")\n",
        "            return\n",
        "\n",
        "        # In SympScan: Column 0 is Disease, Col 1+ are Symptoms\n",
        "        self.raw_labels = data.iloc[:, 0].astype(str)\n",
        "        self.features_df = data.iloc[:, 1:]\n",
        "\n",
        "        # Convert 0/1 features to Float32\n",
        "        self.X = self.features_df.values.astype(np.float32)\n",
        "\n",
        "        # Encode Labels (Disease Names -> 0, 1, 2...)\n",
        "        if label_encoder is None:\n",
        "            self.label_encoder = LabelEncoder()\n",
        "            self.y = self.label_encoder.fit_transform(self.raw_labels)\n",
        "        else:\n",
        "            self.label_encoder = label_encoder\n",
        "            self.y = self.label_encoder.transform(self.raw_labels)\n",
        "\n",
        "        self.symptom_names = list(self.features_df.columns)\n",
        "        self.disease_names = list(self.label_encoder.classes_)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx], dtype=torch.long)\n",
        "\n",
        "# 2. Initialize 'train_ds' (This creates the missing variable)\n",
        "# Ensure the path matches where your 'Diseases_and_Symptoms_dataset.csv' is located.\n",
        "csv_path = \"/content/drive/MyDrive/Dataset/SympScan/Diseases_and_Symptoms_dataset.csv\"\n",
        "\n",
        "# If you are using Google Drive, uncomment and update this line:\n",
        "# csv_path = \"/content/drive/MyDrive/Dataset/SympScan/Diseases_and_Symptoms_dataset.csv\"\n",
        "\n",
        "print(f\"Loading dataset from: {csv_path} ...\")\n",
        "try:\n",
        "    train_ds = SymptomDiseaseDataset(csv_path)\n",
        "    print(\"‚úÖ Success! 'train_ds' is now defined.\")\n",
        "    print(f\"   - Symptoms found: {len(train_ds.symptom_names)}\")\n",
        "    print(f\"   - Diseases found: {len(train_ds.disease_names)}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Could not find file at {csv_path}. Please check the path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxN1VgJVL6qe",
        "outputId": "11df244f-eb28-40dd-c182-78659ffe2a04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from: /content/drive/MyDrive/Dataset/SympScan/Diseases_and_Symptoms_dataset.csv ...\n",
            "‚úÖ Success! 'train_ds' is now defined.\n",
            "   - Symptoms found: 230\n",
            "   - Diseases found: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# üß™ EXECUTE LIVE TESTS\n",
        "# ==========================================\n",
        "\n",
        "def test_scenario(model, symptoms, expected_disease_hint):\n",
        "    print(f\"\\n==================================================\")\n",
        "    print(f\"üßê TESTING SCENARIO: {expected_disease_hint}\")\n",
        "    print(f\"==================================================\")\n",
        "\n",
        "    # Run diagnosis\n",
        "    diagnose_and_prescribe(model, symptoms)\n",
        "\n",
        "# --- Scenario 1: Mental Health Check ---\n",
        "test_scenario(\n",
        "    student,\n",
        "    ['anxiety and nervousness', 'shortness of breath', 'palpitations', 'chest tightness'],\n",
        "    \"Expected: Panic Disorder\"\n",
        ")\n",
        "\n",
        "# --- Scenario 2: Common Infection ---\n",
        "test_scenario(\n",
        "    student,\n",
        "    ['sore throat', 'fever', 'difficulty in swallowing', 'swollen or red tonsils'],\n",
        "    \"Expected: Strep Throat\"\n",
        ")\n",
        "\n",
        "# --- Scenario 3: Physical Condition ---\n",
        "test_scenario(\n",
        "    student,\n",
        "    ['back pain', 'neck pain', 'weakness', 'leg pain'],\n",
        "    \"Expected: Herniated Disk / Spondylosis\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-euXG8fITh-",
        "outputId": "64957cba-2710-4afa-ae01-1151eb386502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üßê TESTING SCENARIO: Expected: Panic Disorder\n",
            "==================================================\n",
            "\n",
            "--- Analyzing Symptoms: ['anxiety and nervousness', 'shortness of breath', 'palpitations', 'chest tightness'] ---\n",
            "  >> Diagnosis: PANIC DISORDER (97.67%)\n",
            "\n",
            "--- üíä TREATMENT PLAN FOR: PANIC DISORDER ---\n",
            "ü•ó Diet:        ['Magnesium-rich foods (spinach, pumpkin seeds, almonds)', 'Omega-3 fatty acids (salmon, flaxseeds, walnuts)', 'Complex carbs (oats, quinoa)', 'Green tea (L-theanine)', 'Limit caffeine and sugar']\n",
            "üíä Medication:  ['SSRIs (e.g., Sertraline, Fluoxetine)', 'Benzodiazepines (e.g., Clonazepam, Alprazolam)', 'SNRIs (e.g., Venlafaxine)', 'Beta-blockers', 'Cognitive Behavioral Therapy (CBT)']\n",
            "üèÉ Workout:     [\"Deep breathing exercises: Calm your mind by focusing on slow, deep breaths\", \"Yoga: Combines breathing and movement for relaxation\", \"Mindfulness meditation: Helps reduce anxiety by staying present\", \"Regular aerobic exercise: Boosts mood and reduces stress\"]\n",
            "‚ö†Ô∏è Precautions: Practice deep breathing, Avoid caffeine, Follow therapy plan, Seek support from loved ones\n",
            "\n",
            "==================================================\n",
            "üßê TESTING SCENARIO: Expected: Strep Throat\n",
            "==================================================\n",
            "\n",
            "--- Analyzing Symptoms: ['sore throat', 'fever', 'difficulty in swallowing', 'swollen or red tonsils'] ---\n",
            "  >> Diagnosis: STREP THROAT (95.25%)\n",
            "\n",
            "--- üíä TREATMENT PLAN FOR: STREP THROAT ---\n",
            "ü•ó Diet:        ['Soft foods (soups, mashed potatoes)', 'Warm teas (ginger, chamomile)', 'Hydration', 'Avoid acidic or spicy foods', 'Vitamin C-rich foods (oranges, strawberries)']\n",
            "üíä Medication:  ['Penicillin', 'Amoxicillin', 'Azithromycin (if allergic to penicillin)', 'Analgesics (e.g., Acetaminophen)', 'Salt water gargles']\n",
            "üèÉ Workout:     [\"Rest: Until infection clears\", \"Avoid cardio: While febrile or sore throat\", \"Walking: Gradually after symptoms ease\", \"Hydration and vocal rest after workouts\"]\n",
            "‚ö†Ô∏è Precautions: Complete full antibiotic course, Avoid sharing utensils, Get adequate rest, Drink warm fluids\n",
            "\n",
            "==================================================\n",
            "üßê TESTING SCENARIO: Expected: Herniated Disk / Spondylosis\n",
            "==================================================\n",
            "\n",
            "--- Analyzing Symptoms: ['back pain', 'neck pain', 'weakness', 'leg pain'] ---\n",
            "  >> Diagnosis: CHRONIC BACK PAIN (60.38%)\n",
            "\n",
            "--- üíä TREATMENT PLAN FOR: CHRONIC BACK PAIN ---\n",
            "ü•ó Diet:        ['Anti-inflammatory foods (berries, turmeric)', 'Calcium and Vitamin D (milk, cheese, eggs)', 'Magnesium-rich foods (nuts, leafy greens)', 'Omega-3s (salmon)']\n",
            "üíä Medication:  ['NSAIDs', 'Muscle relaxants', 'Physical therapy', 'Epidural steroid injections', 'Chronic pain management (e.g., TENS, acupuncture)']\n",
            "üèÉ Workout:     [\"Core stabilization: Essential for support\", \"Water aerobics: Minimal spinal impact\", \"Stretching: Hamstrings, hips, and back\", \"Avoid high-impact sports\"]\n",
            "‚ö†Ô∏è Precautions: Maintain proper posture, Regular stretching, Use ergonomic furniture, Avoid lifting heavy objects\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Scenario 4: Respiratory Condition ---\n",
        "# Testing specifically for Asthma keywords\n",
        "test_scenario(\n",
        "    student,\n",
        "    ['wheezing', 'cough', 'shortness of breath', 'chest tightness'],\n",
        "    \"Expected: Asthma\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQa1ekQkM0kO",
        "outputId": "014e2474-128c-46b2-b15b-25992e8bfead"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üßê TESTING SCENARIO: Expected: Asthma\n",
            "==================================================\n",
            "\n",
            "--- Analyzing Symptoms: ['wheezing', 'cough', 'shortness of breath', 'chest tightness'] ---\n",
            "  >> Diagnosis: CHRONIC OBSTRUCTIVE PULMONARY DISEASE (COPD) (41.46%)\n",
            "\n",
            "--- üíä TREATMENT PLAN FOR: CHRONIC OBSTRUCTIVE PULMONARY DISEASE (COPD) ---\n",
            "ü•ó Diet:        ['Anti-inflammatory foods (turmeric, ginger, berries)', 'Omega-3 fatty acids (wild salmon, walnuts)', 'High-protein foods (chicken, beans)', 'Vitamin C-rich foods (oranges, broccoli)', 'Hydration']\n",
            "üíä Medication:  ['Bronchodilators (e.g., Salbutamol)', 'Inhaled corticosteroids', 'Phosphodiesterase-4 inhibitors (e.g., Roflumilast)', 'Oxygen therapy', 'Antibiotics during exacerbations']\n",
            "üèÉ Workout:     [\"Pursed-lip breathing: Improve oxygen use\", \"Walking: Build endurance safely\", \"Stationary biking: Low strain on lungs\", \"Pulmonary rehabilitation exercises: Doctor-guided regimens\"]\n",
            "‚ö†Ô∏è Precautions: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3RFzOxp8cCu"
      },
      "source": [
        "## 8. EXPORTING ARTIFACTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7ERUB7B8cCu",
        "outputId": "f2edd55a-1e32-47ea-f07a-44d41949f096"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[7/7] Saving Artifacts...\n",
            "üíæ Saved: 'mexar_nano_student.pth'\n",
            "üíæ Saved: 'mexar_metadata.pkl'\n",
            "\n",
            "üî• MEXAR Nano Update Complete.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[7/7] Saving Artifacts...\")\n",
        "\n",
        "# 1. Save Student Weights (The Nano Model)\n",
        "torch.save(student.state_dict(), \"mexar_nano_student.pth\")\n",
        "\n",
        "# 2. Save Complete Metadata\n",
        "# This is crucial for the app. It connects the 0/1 inputs to real symptom names,\n",
        "# and connects the output integers 0,1,2 to real disease names + advice.\n",
        "metadata = {\n",
        "    \"model_type\": \"Knowledge Distillation (Teacher-Student)\",\n",
        "    \"input_dim\": input_dim,\n",
        "    \"num_classes\": num_classes,\n",
        "    \"symptom_names\": symptom_names,     # List of strings [symptom1, symptom2...]\n",
        "    \"disease_names\": disease_names,     # List of strings [disease1, disease2...] (LabelEncoder classes)\n",
        "    \"knowledge_base\": full_knowledge_base # The merged CSV data\n",
        "}\n",
        "\n",
        "with open(\"mexar_metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"üíæ Saved: 'mexar_nano_student.pth'\")\n",
        "print(\"üíæ Saved: 'mexar_metadata.pkl'\")\n",
        "print(\"\\nüî• MEXAR Nano Update Complete.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}